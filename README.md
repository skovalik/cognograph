# Cognograph

**Connect your thinking. The AI follows.**

A spatial canvas where AI conversations, notes, tasks, and projects live as connected nodes. Draw a connection between two nodes and the AI reads both. Your layout is your prompt engineering.

<!-- TODO: Add demo GIF showing: create note → create conversation → connect them → AI responds with note context -->

---

## The Problem

You have 47 ChatGPT tabs open. One has your competitor analysis. One has your brand guidelines. One has that pricing breakdown you spent an hour on. You can't remember which is which, and every new conversation starts from zero.

AI chat interfaces treat each conversation as an island. But your work isn't islands — it's a graph. Ideas reference other ideas. Research feeds into planning. Decisions depend on context from three different threads. You end up copy-pasting between tabs, rewriting system prompts, and re-explaining things the AI already knows (somewhere, in some other conversation you can't find).

Cognograph fixes this by making the connections visible and functional. Put your thinking on a canvas. Connect the pieces. The AI reads the connections and understands what's relevant — no copy-pasting, no prompt engineering, no configuration. **The act of organizing your work IS the act of programming the AI.**

---

## Features

### Context Injection (the core innovation)

When you draw an edge between nodes, the system walks the graph via breadth-first traversal on inbound edges and builds the AI's context window automatically. Notes become reference material, Tasks become constraints, Projects define scope. Edge direction and weight control priority. Token budgets prevent runaway costs.

You don't write system prompts. You connect things.

### Semantic Zoom (Rich Node Depth)

The canvas adapts to your zoom level automatically. No mode switching, no settings — just zoom and the right information appears:

| Zoom Level | What You See |
|------------|-------------|
| **Far (L0)** | Colored rectangles with icon + title. Cluster summary bubbles. Weak edges hidden. |
| **Overview (L1)** | Property badges and metadata appear. Edge labels become readable. |
| **Mid (L2)** | Full node content progressively reveals. All connection types visible. |
| **Close (L3)** | Complete content, descriptions, property panels, all handles. |
| **Expanded (L4)** | In-place artboard mode — node expands on canvas for deep work. |

### 8 Node Types

| Node | Purpose |
|------|---------|
| **Conversation** | Chat with AI, context-aware via connections |
| **Note** | Static content — research, guidelines, references (10 visual modes) |
| **Task** | Actionable items with status and priority |
| **Project** | Group and scope related nodes |
| **Artifact** | Files dropped onto the canvas or generated by AI |
| **Text** | Rich-text freeform blocks (TipTap editor) |
| **Action** | Spatial trigger zones on the canvas |
| **Workspace** | Top-level settings and defaults |

### Canvas Intelligence

- **Canvas Districts** — Named spatial zones (e.g., "Research," "Implementation") with visual tinting. Organize large canvases into semantic neighborhoods.
- **Canvas Table of Contents** — `Ctrl+Shift+T` opens a searchable outline of your entire canvas. Jump to any district, landmark, or node group instantly.
- **Context Visualization** — See which nodes feed context into which. Hover to reveal the BFS traversal radius as a visual scope badge.
- **Cognitive Load Meter** — Real-time indicator of canvas complexity. Warns when you have too many visible nodes — "Consider zooming in or filtering."
- **Calm Mode** — Strips away all secondary UI (badges, meters, overlays) for distraction-free work.
- **Session Re-Entry** — When you return after being away, Cognograph surfaces your most recently worked-on nodes so you can pick up where you left off.
- **Landmark Nodes** — Mark key reference nodes that always get priority in context injection regardless of graph distance.

### Z-Key Quick Navigation

Hold **Z** to get a full-screen bird's-eye minimap overlay. Drag a rectangle over any area, release Z — the viewport smoothly animates to that selection. Jump between distant canvas regions in under a second.

### Claude Code Integration

- **3-tier session mapping** — Automatic (env var), manual (right-click link), or prompted (toast notification) linking between Claude Code sessions and canvas nodes.
- **Execution status overlay** — See which nodes have active/idle/completed sessions with depth-of-field blur on non-active nodes.
- **Dispatch workflows** — Right-click to trigger predefined workflows on nodes.
- **Embedded terminal** — PTY-backed terminal sessions with scrollback replay (requires `node-pty`).

### More Capabilities

- **Spatial Triggers** — Drag a node into an Action zone and it auto-summarizes, auto-categorizes, or runs custom logic. Canvas position as a trigger condition.
- **Plan-Preview-Apply** — Ask the AI to reorganize your canvas. Ghost nodes show what's about to change. Refine conversationally. Apply atomically with per-operation undo.
- **Token Budget Tracking** — See how much context each connection contributes. Stay under limits without guessing.
- **20 Ambient Effects** — Living Grid, Topography, Aurora, Starfield, Fireflies, Rain, Particles, Iridescence, and more. Pure aesthetics, GPU-friendly.
- **Agent Mode** — Conversation nodes can act autonomously — creating, editing, and connecting nodes on the canvas based on your instructions.
- **Notion Integration** — Plugin-based sync: workspace-level metadata and per-node push/pull to Notion databases.
- **Local-First** — No account. No cloud. Your data stays on your machine. Workspaces save to disk automatically.
- **MCP Server** — Expose your workspace to Claude Code and other MCP-compatible agents.
- **Plugin System** — Extensible architecture for third-party integrations with IPC security sandboxing.

---

## Quick Start

```bash
git clone https://github.com/skovalik/cognograph.git
cd cognograph
npm install
npm run dev
```

On first conversation, the app will ask for an API key. Paste it in and you're running.

### Requirements

- Node.js 18+
- npm 9+
- An API key from at least one provider (see below)

### API Key Setup

Open Settings (gear icon, top-right) and add keys for any provider you want to use:

- **Anthropic** — Claude models (default)
- **OpenAI** — GPT-4o, o1, etc.
- **Google Gemini** — Gemini Pro / Flash

Each conversation node can use a different provider. Set per-node or use workspace defaults.

---

## Supported Providers

| Provider | Status | Models |
|----------|--------|--------|
| Anthropic | Supported | Claude Sonnet, Opus, Haiku |
| OpenAI | Supported | GPT-4o, o1-mini, o1, etc. |
| Google Gemini | Supported | Gemini Pro, Flash |

---

## Tech Stack

| Layer | Technology |
|-------|------------|
| Desktop | Electron 33 |
| Frontend | React 18, TypeScript 5 |
| Canvas | React Flow (xyflow) |
| State | Zustand + Immer |
| Styling | Tailwind CSS 3 |
| Rich Text | TipTap |
| LLM SDKs | Anthropic, OpenAI, Google Generative AI |
| 3D/Effects | Three.js, R3F, OGL |
| Build | electron-vite, Vite |
| Testing | Vitest (1,305 tests), Playwright |

---

## Project Structure

```
src/
├── main/           # Electron main process (IPC, file I/O, LLM calls, MCP server)
├── preload/        # IPC bridge (type-safe API surface)
├── renderer/       # React app
│   ├── components/ # UI components + 8 node types
│   ├── stores/     # Zustand stores (37 stores)
│   ├── services/   # Agent tools, action executor, extraction
│   └── utils/      # Mutation executor, helpers
├── plugins/        # Plugin system (Notion, etc.)
└── shared/         # Types shared across all processes
```

---

## The Story

I have ASD and severe combined-type ADHD. Files in folders don't work for me. Linear chat doesn't work for me. I need to see everything spatially, with connections I can trace with my eyes.

When I started working with AI daily — dozens of conversations, research, code, planning — I hit a wall. 47 tabs. Context lost between them. Redoing work because I couldn't find where I'd already done it. The tools weren't built for how I think.

Cognograph started as a tool for my own brain. Turns out a lot of brains need it — neurodivergent or not, most people think better when they can see everything at once.

---

## Contributing

Contributions are welcome. The codebase is TypeScript throughout with strict types, Zustand for state, and React Flow for the canvas layer.

Start with `ARCHITECTURE.md` for how the system works and `FEATURES.md` for what it does. `docs/guides/PITFALLS.md` covers the gotchas that'll save you time.

If you're not sure where to start, open an issue.

---

## Documentation

| Document | Purpose |
|----------|---------|
| [FEATURES.md](./FEATURES.md) | Comprehensive feature documentation |
| [ARCHITECTURE.md](./ARCHITECTURE.md) | System design, data flow, components |
| [docs/strategy/VISION.md](./docs/strategy/VISION.md) | Strategy, goals, design philosophy |
| [docs/strategy/NORTH_STAR.md](./docs/strategy/NORTH_STAR.md) | Long-term 3D desktop vision |
| [docs/guides/PITFALLS.md](./docs/guides/PITFALLS.md) | Common bugs and how to avoid them |
| [CONTRIBUTING.md](./CONTRIBUTING.md) | How to contribute |
| [DECISIONS.md](./DECISIONS.md) | Decision log with rationale |
| [CHANGELOG.md](./CHANGELOG.md) | Session-by-session work history |

---

## Patent Status

Patent pending. Four provisional patent applications filed February 2026 covering:

1. **Context Injection** — Graph-based automatic context assembly for AI conversations
2. **Spatial Agent Orchestration** — Canvas topology controlling agent permissions and behavior
3. **Spatial Triggers** — Canvas position as a trigger condition for automated workflows
4. **Plan-Preview-Apply** — Conversational plan refinement with ghost-node preview and atomic execution

---

## License

[AGPL-3.0](./LICENSE) with [Defensive Patent Pledge](./PATENTS). Patent pending.

**Free for personal use.** Commercial use requires a commercial license — [contact us](mailto:stefan@aurochs.agency) for details.

If you distribute modified versions or run them as a network service, AGPL-3.0 requires you to release your source code under the same license.

---

*Built by Stefan Kovalik in San Francisco. Designed to close the gap between how people think and how AI tools expect them to.*
